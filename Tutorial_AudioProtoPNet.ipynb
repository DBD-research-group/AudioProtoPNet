{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "tutorial-introduction",
   "metadata": {},
   "source": [
    "# AudioProtoPNet Inference Tutorial\n",
    "\n",
    "Welcome to the **AudioProtoPNet** inference tutorial! This notebook shows you how to perform audio classification using pre-trained [AudioProtoPNet](https://github.com/DBD-research-group/AudioProtoPNet) models. These models are trained to identify bird species from audio recordings.\n",
    "\n",
    "## About AudioProtoPNet Models\n",
    "AudioProtoPNet is a prototype-based neural network architecture. Here, each class in the dataset has a specified number of prototypes that help the model discern audio features relevant to identifying that class.\n",
    "\n",
    "There are four variants of AudioProtoPNet models:\n",
    "- **AudioProtoPNet-1**: 1 prototype per class (9,736 total prototypes)\n",
    "- **AudioProtoPNet-5**: 5 prototypes per class (48,680 total prototypes)\n",
    "- **AudioProtoPNet-10**: 10 prototypes per class (97,360 total prototypes)\n",
    "- **AudioProtoPNet-20**: 20 prototypes per class (194,720 total prototypes)\n",
    "\n",
    "In this notebook, we demonstrate how to load the model, preprocess audio data, and generate predictions. **By default, we now use** AudioProtoPNet-20, but you can switch to any of the four variants by adjusting the relevant checkpoint path and number of prototypes.\n",
    "\n",
    "For further insights into the modelâ€™s architecture and evaluation, please refer to the original research paper available [here](https://www.sciencedirect.com/science/article/pii/S1574954125000901).\n",
    "\n",
    "## Key Points in This Tutorial\n",
    "1. **Resampling**: We standardize audio to a 32 kHz sampling rate.\n",
    "2. **Spectrogram Generation**: We convert the audio wave into a spectrogram using short-time Fourier transform (STFT) and Mel scaling.\n",
    "3. **Normalization (z-standardization)**: We subtract a mean and divide by a standard deviation, calculated from the training data.\n",
    "4. **Model Flexibility**: Although the model was trained on 5-second clips, it can be applied to audio of arbitrary length.\n",
    "\n",
    "> **Important**: The spectrogram parameters shown here must be used **exactly** (e.g., `n_fft`, `hop_length`, `n_mels`, `n_stft`, and the standardization values). Changing these parameters can lead to performance degradation.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "package-versions",
   "metadata": {},
   "source": [
    "## Package Versions Used in This Notebook\n",
    "\n",
    "Below are the main packages and versions used in this tutorial:\n",
    "\n",
    "- **librosa**: 0.10.0\n",
    "- **numpy**: 1.24.4\n",
    "- **torch**: 2.3.1\n",
    "- **torchaudio**: 2.3.1\n",
    "- **matplotlib**: 3.7.2\n",
    "- **ipython**: 8.25.0\n",
    "- **datasets**: 2.18.0\n",
    "- **birdset**: 0.2.0\n",
    "- **huggingface_hub**: 0.24.6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacd14e0-229d-4a87-95c4-166bac6bfd3d",
   "metadata": {},
   "source": [
    "### Import necessary Python packages and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d0af43-4936-4d5f-9556-f31565e8ba44",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Here we import all necessary libraries:\n",
    "# - librosa and torchaudio for audio processing\n",
    "# - numpy and torch for numerical computation\n",
    "# - matplotlib for plotting\n",
    "# - IPython.display for audio playback\n",
    "# - hf_hub_download for pulling checkpoints directly from Hugging Face\n",
    "# - our custom modules for dataset and model definitions\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchaudio import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "import datasets\n",
    "from audioprotopnet.modules.baselines.convnext import ConvNextClassifier\n",
    "from audioprotopnet.modules.ppnet.ppnet import PPNet\n",
    "from audioprotopnet.modules.checkpoint_loading import load_state_dict\n",
    "from birdset.datamodule.components import augmentations\n",
    "from huggingface_hub import hf_hub_download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-selection",
   "metadata": {},
   "source": [
    "### Choosing Your AudioProtoPNet Model Variant\n",
    "Below, we define our model parameters. Notice that we currently have default values set for **AudioProtoPNet-20**. If you want to use a different variant (such as AudioProtoPNet-1, -5, or -10), simply change the `variant` variable below.\n",
    "\n",
    "The appropriate Hugging Face repo name, checkpoint filename, and number of prototypes will be selected automatically.\n",
    "\n",
    "Example:\n",
    "```python\n",
    "variant = 5  # can be 1, 5, 10, or 20\n",
    "```\n",
    "\n",
    "Then just run the cell to load everything!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353a4a02-52c2-41ea-b800-e6b21476a096",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Set path variables for audio files and configure the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc26faa8-4c90-4fea-b7d2-552fdc2f65cb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# User selection for the AudioProtoPNet variant:\n",
    "variant = 20  # Options: 1, 5, 10, or 20.\n",
    "\n",
    "# Define a lookup that maps the variant to:\n",
    "# - The HF repo name\n",
    "# - The checkpoint filename\n",
    "# - The total number of prototypes\n",
    "variant_to_info = {\n",
    "    1: {\n",
    "        \"ckpt_repo\": \"DBD-research-group/AudioProtoPNet-1-BirdSet-XCL\",\n",
    "        \"ckpt_filename\": \"AudioProtoPNet-1.ckpt\",\n",
    "        \"num_prototypes\": 9736\n",
    "    },\n",
    "    5: {\n",
    "        \"ckpt_repo\": \"DBD-research-group/AudioProtoPNet-5-BirdSet-XCL\",\n",
    "        \"ckpt_filename\": \"AudioProtoPNet-5.ckpt\",\n",
    "        \"num_prototypes\": 48680\n",
    "    },\n",
    "    10: {\n",
    "        \"ckpt_repo\": \"DBD-research-group/AudioProtoPNet-10-BirdSet-XCL\",\n",
    "        \"ckpt_filename\": \"AudioProtoPNet-10.ckpt\",\n",
    "        \"num_prototypes\": 97360\n",
    "    },\n",
    "    20: {\n",
    "        \"ckpt_repo\": \"DBD-research-group/AudioProtoPNet-20-BirdSet-XCL\",\n",
    "        \"ckpt_filename\": \"AudioProtoPNet-20.ckpt\",\n",
    "        \"num_prototypes\": 194720\n",
    "    }\n",
    "}\n",
    "\n",
    "# Retrieve the info for the chosen variant\n",
    "chosen_info = variant_to_info[variant]\n",
    "ckpt_repo = chosen_info[\"ckpt_repo\"]\n",
    "ckpt_filename = chosen_info[\"ckpt_filename\"]\n",
    "num_prototypes = chosen_info[\"num_prototypes\"]\n",
    "\n",
    "print(f\"You have selected AudioProtoPNet-{variant}.\")\n",
    "print(f\"HuggingFace Repo: {ckpt_repo}\")\n",
    "print(f\"Checkpoint Filename: {ckpt_filename}\")\n",
    "print(f\"Number of Prototypes: {num_prototypes}\")\n",
    "\n",
    "# We define the number of classes (n_classes=9736 for BirdSet)\n",
    "n_classes = 9736  # number of classes (bird species)\n",
    "\n",
    "# Instantiate the backbone model (ConvNeXt), which is our feature extractor.\n",
    "backbone = ConvNextClassifier(\n",
    "    num_classes=n_classes,\n",
    "    num_channels=1,\n",
    "    embedding_size=1024,        # Must match prototype_shape.channels.\n",
    "    backbone_mode=True,\n",
    "    checkpoint=\"facebook/convnext-base-224-22k\",\n",
    "    local_checkpoint=None,\n",
    "    cache_dir=None,\n",
    "    pretrain_info=None,\n",
    ")\n",
    "\n",
    "# The prototype shape must match our chosen number of prototypes.\n",
    "prototype_shape = {\n",
    "    \"num_prototypes\": num_prototypes,\n",
    "    \"channels\": 1024,\n",
    "    \"height\": 1,\n",
    "    \"width\": 1,\n",
    "}\n",
    "\n",
    "# Instantiate the PPNet model using the backbone.\n",
    "model = PPNet(\n",
    "    backbone_model=backbone,\n",
    "    prototype_shape=prototype_shape,\n",
    "    num_classes=n_classes,\n",
    "    topk_k=1,\n",
    "    margin=None,\n",
    "    init_weights=False,\n",
    "    add_on_layers_type=\"upsample\",\n",
    "    incorrect_class_connection=None,\n",
    "    correct_class_connection=1.0,\n",
    "    bias_last_layer=-2.0,\n",
    "    non_negative_last_layer=True,\n",
    "    embedded_spectrogram_height=None,\n",
    "    pretrain_info=None,\n",
    ")\n",
    "\n",
    "# Download the checkpoint from Hugging Face:\n",
    "ckpt_path = hf_hub_download(repo_id=ckpt_repo, filename=ckpt_filename)\n",
    "\n",
    "# Load the model weights from the checkpoint.\n",
    "state_dict = load_state_dict(ckpt_path)\n",
    "model.load_state_dict(state_dict, strict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resampling-note",
   "metadata": {},
   "source": [
    "### Note on Resampling to 32 kHz\n",
    "The AudioProtoPNet models are trained on audio resampled to **32,000 Hz**. When using your own audio files, always ensure they are resampled to 32 kHz before feeding them into the model. Below, we show how to load and resample a built-in example audio (of a European Robin) in one step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0cd8fb-1f61-43fe-abd0-d9d62d933d3b",
   "metadata": {},
   "source": [
    "## Loading an Example Birdsong"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23881e6-aed1-462e-83e3-27643b3669e6",
   "metadata": {},
   "source": [
    "#### Load the audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739d4628-9ee6-4a6e-95d3-7cf7e47a60a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 'librosa.ex' loads a built-in example audio of a European Robin.\n",
    "sample_rate = 32000\n",
    "audio_path = librosa.ex('robin')\n",
    "\n",
    "label = \"eurrob1\"  # The eBird label for the European Robin.\n",
    "\n",
    "# Load the audio file into 'audio' with our chosen sample_rate of 32 kHz.\n",
    "audio, sample_rate = librosa.load(audio_path, sr=sample_rate)\n",
    "Audio(audio, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1260b3b1-78b7-4b61-8a7c-718828921c68",
   "metadata": {},
   "source": [
    "#### Visualizing the Audio Waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4d46df-c0e3-437c-bf43-7bf5eb2a3acd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "librosa.display.waveshow(y=audio, sr=sample_rate)\n",
    "plt.title(label)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude (normalized)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c300753a-56c5-4e8c-9c9c-a4727e351e3d",
   "metadata": {},
   "source": [
    "## Converting the Audio Signal into a Spectrogram\n",
    "\n",
    "In order to use the AudioProtoPNet model, we must convert the raw audio waveform into a Mel-scaled spectrogram. The steps we use are:\n",
    "\n",
    "1. **Spectrogram Transformation** (`transforms.Spectrogram`):\n",
    "   - Uses STFT with a specific `n_fft` (window size) and `hop_length` (stride).\n",
    "2. **Mel Scaling** (`transforms.MelScale`):\n",
    "   - Converts frequencies to the Mel scale.\n",
    "3. **dB Conversion** (`augmentations.PowerToDB()`):\n",
    "   - Applies a logarithmic transform to the power spectrogram.\n",
    "4. **Z-Standardization**:\n",
    "   - We subtract a mean and divide by a standard deviation. These values (`mean` and `std`) come from the training data statistics.\n",
    "\n",
    "Although the AudioProtoPNet was trained on 5-second clips, **you can use audio of any length**. Just note that the model processes the entire spectrogram.\n",
    "\n",
    "> **Warning**: The parameters for preprocessing below (e.g., `n_fft=2048`, `hop_length=256`, `n_mels=256`, `n_stft=1025`, `mean=-13.369`, `std=13.162`) **must** be used. Changing them may result in decreased performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792da463-bcab-4ab9-8d15-85f6c7f91f0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Below are the parameters used for spectrogram generation.\n",
    "# - n_fft=2048: Window size for STFT\n",
    "# - hop_length=256: Step between windows\n",
    "# - n_mels=256: Number of Mel frequency bins\n",
    "# - n_stft=1025: The number of FFT bins used internally by MelScale\n",
    "# - sample_rate=32000: Our target sampling rate\n",
    "# 'mean' and 'std' are the training data statistics for z-standardization.\n",
    "spec_transform = transforms.Spectrogram(n_fft=2048, hop_length=256, power=2.0)\n",
    "mel_scale = transforms.MelScale(n_mels=256, sample_rate=32000, n_stft=1025)\n",
    "db_scale = augmentations.PowerToDB()\n",
    "mean = -13.369\n",
    "std = 13.162"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6168b8cb-d59f-43e1-912f-b2fcc5284c1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform the actual transformation to get a normalized spectrogram.\n",
    "audio_tensor = torch.as_tensor(audio)\n",
    "spec_gram = spec_transform(audio_tensor)\n",
    "mel_spec = mel_scale(spec_gram)\n",
    "mel_spec = db_scale(mel_spec)\n",
    "mel_spec_norm = (mel_spec - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53537247-97bc-492f-b213-03c72f091ffa",
   "metadata": {},
   "source": [
    "#### Visualizing the Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667789af-1954-4dcf-a0af-3ffaac892667",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.imshow(mel_spec_norm, origin='lower')\n",
    "plt.title('Spectrogram ' + label)\n",
    "plt.xlabel('Time (ms)')\n",
    "plt.ylabel('Frequency (Mels)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-inference",
   "metadata": {},
   "source": [
    "## Classifying the Bird Species\n",
    "Now that we have our preprocessed spectrogram, we can feed it into the model and obtain a prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59906424-e3c6-495a-8f81-22a8a167f479",
   "metadata": {},
   "source": [
    "### Model Evaluation Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fcc6ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# We set the model to evaluation mode,\n",
    "# which turns off certain layers like dropout.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610fa758-6b55-4b0d-a5ef-ac7337ad2acd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Adjust the tensor dimensions for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96de82a-4b63-4191-a37a-a3afad9dd7f3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The model expects a batch of data. We add two dimensions:\n",
    "# 1) a batch dimension\n",
    "# 2) a channel dimension (we have 1 channel)\n",
    "batch = mel_spec_norm.unsqueeze(0).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d85dae-6885-474f-ae7f-ac14058a60e5",
   "metadata": {},
   "source": [
    "#### Forward pass through the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa2278d-0250-4db3-8539-79dada272d5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outputs = model(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b17cf4-338f-4148-907b-87064c21f467",
   "metadata": {},
   "source": [
    "#### Convert the model outputs to confidence scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726ff6d0-16cc-485a-b797-209e3fed0696",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "confidence_scores = torch.sigmoid(outputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e1d625-a59f-41cd-8b44-fa7debd584f7",
   "metadata": {},
   "source": [
    "#### Identifying the species class with the highest confidence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed083ae8-a204-4287-8cc9-9eaf6d4e2a94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prediction = np.argmax(confidence_scores.detach().numpy())\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bb4bea-5670-4917-ac99-884ae053c9e7",
   "metadata": {},
   "source": [
    "#### Lookup the predicted bird species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac74d515-c955-4605-b623-ed1813c4311c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ebird_codes_list = datasets.load_dataset_builder(\"DBD-research-group/BirdSet\", \"XCL\", trust_remote_code=True).info.features[\"ebird_code\"]\n",
    "label_to_category_mapping = dict(enumerate(ebird_codes_list.names))\n",
    "predicted_label = label_to_category_mapping[prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68588907-5359-4088-98c0-fe26499c3b3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'Selected species: {label}')\n",
    "print(f'Predicted species: {predicted_label}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we:\n",
    "1. Loaded a pre-trained AudioProtoPNet model (defaulting to AudioProtoPNet-20 in this example).\n",
    "2. Resampled the audio to 32 kHz.\n",
    "3. Converted the audio to a Mel-scaled spectrogram and applied z-standardization.\n",
    "4. Obtained a bird species prediction from the model.\n",
    "\n",
    "To adapt this notebook for your own data:\n",
    "- Provide your own audio file path and ensure the audio is resampled to 32 kHz.\n",
    "- **Use the same spectrogram transformation parameters** (`n_fft=2048`, `hop_length=256`, `n_mels=256`, `n_stft=1025`, `mean=-13.369`, and `std=13.162`). Changing them can degrade performance.\n",
    "- Load the checkpoint of your preferred AudioProtoPNet variant.\n",
    "\n",
    "We hope this tutorial gives you a clear understanding of how to use AudioProtoPNet for inference!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (audioprotopnet-env)",
   "language": "python",
   "name": "audioprotopnet-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
